---
title: "Equity text explorations"
output: html_notebook
---

# Equity text explorations - 

## Required packages

```{r}
# required libraries

library(here) # 
library(tidyverse) # for general data wrangling
library(rvest) # for webscraping
library(polite) # for polite interactions with webserveres
library(fs)
library(pdftools) # for PDF text extraction
library(tm) # for text mining
library(quanteda) # analysis of textual data  
library(quanteda.textstats)
library(tidytext) #a tidy data model for textual analysis
library(writexl) # export tables to Excel
library(httr2) #webscraping update and rewrite to httr
library(httr) #webscraping 
library(quanteda.textmodels) # for topic modelling using LDA - but this doesn't seem to be available anymore
library(topicmodels) # to do the topic modelling using LDA
library(proxy) # for calculating cosine and jaccard similarity measures
library(philentropy) # for calculating Jensen-Shannon Divergence
library(heatmaply) # for creating heatmaps of topic distributions
library(scales) # scales 
library(kableExtra) #for tables
library(ldatuning) # for topic coherence
library(SnowballC)
library(spacyr)

# Load data
# load("./.RData")
```

# MDBA northern basin amendments


```{r}
mdba_NB_url <- "https://getinvolved.mdba.gov.au/bp-amendments-submissions/widgets/139364/documents"

# from the polite package, we properly identify ourselves and respect any explicit limits
session <- bow(mdba_NB_url, force = TRUE)

# scrape the page contents
NB_page <- scrape(session)
```

```{r}
download_links <- tibble(link_names = NB_page %>%
                           html_nodes("a")%>%
                           html_text(),
                         link_urls = NB_page %>%
                           html_nodes("a") %>%
                           html_attr('href'))

download_links_docs <- download_links %>%
  filter(str_detect(link_names, "No. [0-9]"))

download_links_docs_subset <- download_links_docs %>%
  slice(c(1:10))

download_links_docs_subset <- download_links_docs_subset %>%
  mutate(link_urls = paste0(link_urls,"/download"),
         link_names = gsub("\\(pdf\\)", "", link_names), 
         link_names = str_trim(link_names),# remove (pdf) from link_names
         link_names = paste0(link_names, ".pdf")) 


download_links_docs<- download_links_docs %>%
  mutate(link_urls = paste0(link_urls,"/download"),
         link_names = gsub("\\(pdf\\)", "", link_names), 
         link_names = str_trim(link_names),# remove (pdf) from link_names
         link_names = paste0(link_names, ".pdf")) 

                           
```

```{r}
# Function to download the PDFs and save in a defined directory

mdba_subdirectory <- "docs_mdba/bp_amendments_docs"

dir.create(mdba_subdirectory, recursive = T, showWarnings = F)

download_pdf <- function(link_urls, link_names) {
  save_path_mdba <- file.path(mdba_subdirectory, link_names)
  httr::GET(link_urls, httr::write_disk(path = save_path_mdba, overwrite = TRUE))
}

# Apply the function to each row of the data frame
pmap(download_links_docs, download_pdf)
##

```

```{r}

# Import the PDFs and build initial corpus

# List all PDF files
mdba_submissions <- list.files(path = "docs_mdba/bp_amendments_docs/", pattern = "*.pdf", full.names = TRUE)

# Function to read PDFs and return a dataframe with the filename and text
read_pdf <- function(file) {
  text <- pdf_text(file)
  clean_text <- gsub("[^[:print:]]", "", text)
  clean_text_collapsed <-  paste(clean_text, collapse = " ")
  tibble(filename = file, text = clean_text_collapsed)
}

# problems loading pdfs - looks like pdftools can't handle some of the pdfs - let's use purrr safely to create a version that doesn't fail when it hits these

# Create a safe version of pdf_text
safe_pdf_text <- safely(pdf_text)

# Apply safe_pdf_text to each file
results <- map(mdba_submissions, safe_pdf_text)

# Check which files caused errors
errors <- map_lgl(results, ~ !is.null(.x$error))

# Print the names of the files that caused errors
print(mdba_submissions[errors])

# here's the result - one pdf file with an error - it won't open manually either - there is a strange management of this single file - it's listed as an html, but can be accesses via two clicks as a pDF- downloaded manually and added to the set, corrupt one removed and replaced with manual download

#> print(mdba_submissions[errors])
# [1] "docs_mdba/bp_amendments//No. 367, Mr Mal Peters (17.2 KB) (html).pdf"




# Apply this function to each PDF file
mdba_sub_text_data <- map_df(mdba_submissions, read_pdf)


```

```{r}

# Create one big concordance

# Regular expressions for key words
mdba_efj <- c("fair\\w*", "unfair\\w*", "equit\\w*", "inequit\\w*", "just\\w*", "unjust\\w*", "injust\\w*")

# Pattern string for use in regex
pattern_string <- paste(mdba_efj, collapse = "|")

mdba_concordance <- mdba_sub_text_data %>%
  corpus()%>%
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>%
  kwic(pattern = pattern_string, valuetype = "regex", window = 30, case_insensitive = TRUE)

```


```{r}
# The single concordance is too big - break it down to individual words as in the above, but write more efficiently than the previous code and save automatically into a defined path

# create subdirectory
mdba_concord_subdirectory <- "docs_mdba/bp_amendments_concordance"

dir.create(mdba_concord_subdirectory, recursive = T, showWarnings = F)


# Define key word patterns
mdba_efj <- c("\\bfair\\w*", "\\bunfair\\w*", "\\bequit\\w*", "\\binequit\\w*", "\\bjust\\w*", "unjust\\w*", "injust\\w*")

# Tokenise the mdba texts and create a corpus
mdba_sub_tokens <- corpus(mdba_sub_text_data$text, docnames = mdba_sub_text_data$filename) %>%
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)

# Iterate over the keyword patterns and export as excel files
map(mdba_efj, ~{
  # Generate KWIC concordance for the current pattern
  kwic_concordance <- mdba_sub_tokens %>%
    kwic(pattern = .x, valuetype = "regex", window = 30, case_insensitive = TRUE) %>%
    as.data.frame()

  # Create a filename for the Excel file
  file_name <- paste0(mdba_concord_subdirectory, "/", "mdba_concordance_", gsub("\\\\w\\*", "", .x), ".xlsx")

  # Export the KWIC concordance to Excel
  write_xlsx(kwic_concordance, path = file_name)
})
```

```{r}
# Co-occurance matrix 

mdba_nb_sub_sentences <- mdba_sub_text_data$text %>%
  tolower() %>%
  paste0(collapse = "") %>%
  unlist() %>%
  tm::removePunctuation()%>%
  stringr::str_squish()

# Create a token object

mdba_nb_sub_token <- mdba_nb_sub_sentences %>%
  tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_remove(stopwords('english')) 

# Compute feature co-occurance matrix

fcm_mdba_nb_sub <- fcm(mdba_nb_sub_token, context = "window", window = 10)

# specify the words of interest
words_of_interest_mdba <- c("fair", "unfair", "equity", "inequity", "inequitable", "justice", "unjust", "injustice")

# check if the words of interest are in the fcm_mat and remove unused words (stops 'subscript out of bounds' error)
words_of_interest_mdba <- words_of_interest_mdba[words_of_interest_mdba %in% rownames(fcm_mdba_nb_sub)]

# Strange output for some words when calculating co-occurance matrices - suspect this is because they occur infrequently  - let's try a frequency list and remove all works with a frequency of less than 10

# Create a frequency list of the tokens
token_freqs_mdba <- table(unlist(tokens(mdba_nb_sub_token)))

# Subset to check the counts for the words of interest
words_of_interest_counts_mdba <- token_freqs_mdba[names(token_freqs_mdba) %in% words_of_interest_mdba]

words_of_interest_mdba <- names(words_of_interest_counts_mdba)[words_of_interest_counts_mdba >= 10]


#Words apart from the words of interest that co-occur
# get the rows of the fcm for the words of interest
fcm_interest_mdba <- fcm_mdba_nb_sub[words_of_interest_mdba, ]

# remove the columns corresponding to the words of interest from these rows
fcm_interest_mdba <- fcm_interest_mdba[, !(colnames(fcm_interest_mdba) %in% words_of_interest)]

# find the most common co-occurring words for each word of interest
top_cooccurring_words_mdba <- apply(fcm_interest_mdba, 1, function(row) {
    names(sort(row, decreasing = TRUE))
})

# find the top N co-occurring words for each word of interest
top_ten_cooccurring_words_mdba <- as.data.frame(apply(fcm_interest_mdba, 1, function(x) {
  cooccurring <- x[!(names(x) %in% words_of_interest_mdba)]
  top_n <- min(10, length(cooccurring))
  names(sort(cooccurring, decreasing = TRUE))[1:top_n]
}))

top_twenty_cooccurring_words_mdba <- as.data.frame(apply(fcm_interest_mdba, 1, function(x) {
  cooccurring <- x[!(names(x) %in% words_of_interest_mdba)]
  top_n <- min(20, length(cooccurring))
  names(sort(cooccurring, decreasing = TRUE))[1:top_n]
}))


# Looking at all toekns in the fcm calculate token frequencies, sort, and convert to data frame

all_feature_counts_df_mdba <- dfm(mdba_nb_sub_token) %>%
  colSums() %>%
  sort(decreasing = TRUE) %>%
  as.data.frame() %>%
  rownames_to_column(var = "word") %>%
  rename(frequency = ".")

# Or just for the top ten

all_feature_counts_df_top_ten <- dfm(mdba_nb_sub_token)  %>% 
  topfeatures() %>% 
  as_tibble(rownames = "token") %>% 
  rename(frequency = value)

# Chart the top 50

all_feature_counts_df_mdba %>%
  head(50) %>%
  ggplot(aes(x = reorder(word, frequency), y = frequency)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(x = "Word", y = "Frequency", title = "Top 50 Word Frequencies") +
  theme_minimal() +
  theme(axis.text = element_text(size = 6))

dir.create("docs_mdba/bp_amendments_other analysis", recursive = T, showWarnings = F)

mdba_output_list <- list(all_feature_counts_df_mdba =all_feature_counts_df_mdba, all_feature_counts_df_top_ten = all_feature_counts_df_top_ten)

output_dir <- "docs_mdba/bp_amendments_other analysis/"

mdba_output_list %>%
      imap(function(df, name) {
    filename <- paste0(output_dir, name, ".xlsx")
    write_xlsx(df, filename)
  })




```

# NSW Legislative Council inquiry floodplain harvesting

## Ensuring individual submission names remain

```{r}
nsw_fph_url <- "https://www.parliament.nsw.gov.au/committees/listofcommittees/Pages/committee-details.aspx?pk=274#tab-hearingsandtranscripts"

session <- bow(nsw_fph_url, force = TRUE)

nsw_fph_page <- scrape(session)

download_links_nsw <- tibble(link_names_nsw = nsw_fph_page %>%
                            html_nodes("td")   %>%
                           html_nodes("a")%>%
                           html_text(),
                         link_urls_nsw = nsw_fph_page %>%
                           html_nodes("td")   %>%
                           html_nodes("a") %>%
                           html_attr('href'))

download_links_docs_nsw <- download_links_nsw %>%
  filter(str_detect(link_names_nsw, "No. [0-9]")) %>%
    mutate(link_names_nsw = str_trim(link_names_nsw, side = c("both"))) 

download_links_docs_subset_nsw <- download_links_docs_nsw %>%
  slice(c(1:10))

```

```{r}

my_urls <- download_links_docs_nsw$link_urls_nsw
save_here <- paste0(download_links_docs_nsw$link_names_nsw, ".pdf")
#mapply(download.file, my_urls, save_here, mode = "wb") 
pmap(list(my_urls, save_here, mode = "wb"), possibly(download.file, NA)) #pmap solves the same problem as mapply, but allows purrr 'possibly' to be used, which like tryCatch ensures the function continues if one element fails

```


## NSW using quanteda corpus function 

```{r}

files_nsw <- list.files(pattern = "pdf$") #creates a vector of PDF file names from PDFs in the same folder (need to move earlier ones out with this model - no path yet - need to improve this code)

submissions_nsw <- lapply(files_nsw, pdf_text)

submissions_nsw_df <- submissions_nsw %>%  #create a dataframe to work with kwic
  map_df(~.x %>%  #ensuring that if there are any issues with different lengths (which there are) that these are left as NAs
           map(~if(length(.)) . else NA) %>% 
           do.call(what = cbind) %>% 
           as_tibble) %>%
  cbind(files_nsw) %>% #linking document names
  unite(submission, V2:V43, na.rm = TRUE) %>% #bringing text from multipage pdfs into a single column
  mutate(submission = str_squish(submission)) %>% # remove white space from beginning and end and double spaces to one space
  rowid_to_column("docname") %>% #create docname reference column
  mutate(docname = str_c("text", docname)) #create docname reference column

```

```{r}
submissions_nsw_v <- unlist(submissions_nsw) # turn documents into a vector for a single corpus

nsw_sub_corpus <- corpus(submissions_nsw_v) # turn vector into a corpus

nsw_sub_corpus_df <- corpus( #corpus from dataframe (note syntax - to retain document boundaries)
  submissions_nsw_df,
  docid_field = "files_nsw",
  text_field = "submission"
)

data_tokens_nsw_sub <- tokens(nsw_sub_corpus_df) #tokenize
kwic(data_tokens_nsw_sub, pattern = "fair") # key words in context test

concordance_just_nsw <- quanteda::kwic(
  data_tokens_nsw_sub,
  pattern = "[a-z]*just[a-z]*",
  valuetype = "regex",
  window = 30) %>%
  as.data.frame() %>%
  filter(!str_detect(keyword, regex("adjust|justin", ignore_case = T)))

write_xlsx(concordance_just_nsw, "concordance_just_nsw.xlsx")

concordance_justice_nsw <- quanteda::kwic(
  data_tokens_nsw_sub,
  pattern = "[a-z]*justice[a-z]*",
  valuetype = "regex",
  window = 30) %>%
  as.data.frame() 

write_xlsx(concordance_justice_nsw, "concordance_justice_nsw.xlsx")

concordance_equit_nsw <- quanteda::kwic(
  data_tokens_nsw_sub,
  pattern = "[a-z]*equit[a-z]*",
  valuetype = "regex",
  window = 30) %>%
  as.data.frame() 

write_xlsx(concordance_equit_nsw, "concordance_equit_nsw.xlsx")

concordance_fair_nsw <- quanteda::kwic(
  data_tokens_nsw_sub,
  pattern = "[a-z]*fair[a-z]*",
  valuetype = "regex",
  window = 30) %>%
  as.data.frame() 

write_xlsx(concordance_fair_nsw, "concordance_fair_nsw.xlsx")

```


## Working with NSW docs as a single document

```{r}

#finding collocations

nsw_sub_sentences <- nsw_sub_corpus%>%
  tolower() %>%
  paste0(collapse = " " ) %>%
  unlist() %>%
  tm::removePunctuation()%>%
  stringr::str_squish()

# Create a token object

nsw_sub_token <- nsw_sub_sentences %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_remove(stopwords('english')) 

# compute feature co-occurrence matrix (fcm)
fcm_nsw_sub <- fcm(nsw_sub_token, context = "window", window = 10)

# specify the words of interest
words_of_interest <- c("fair", "unfair", "equity", "inequity", "inequitable", "justice", "unjust", "injustice")

# filter the fcm to include only co-occurrences with the words of interest
fcm_filtered <- fcm_select(fcm_nsw_sub, pattern = words_of_interest)

# print the filtered co-occurrence matrix
print(fcm_filtered)

#Words apart from the words of interest that co-occur
# get the rows of the fcm for the words of interest
fcm_interest <- fcm_nsw_sub[words_of_interest, ]

# remove the columns corresponding to the words of interest from these rows
fcm_interest <- fcm_interest[, !(colnames(fcm_interest) %in% words_of_interest)]

# find the most common co-occurring words for each word of interest
top_cooccurring_words <- apply(fcm_interest, 1, function(row) {
    names(sort(row, decreasing = TRUE))
})

# find the top N co-occurring words for each word of interest
top_ten_cooccurring_words <- as.data.frame(apply(fcm_interest, 1, function(x) {
  cooccurring <- x[!(names(x) %in% words_of_interest)]
  top_n <- min(10, length(cooccurring))
  names(sort(cooccurring, decreasing = TRUE))[1:top_n]
}))

write_xlsx(top_ten_cooccurring_words, "top_ten_matrix_nsw.xlsx")

top_twenty_cooccurring_words <- as.data.frame(apply(fcm_interest, 1, function(x) {
  cooccurring <- x[!(names(x) %in% words_of_interest)]
  top_n <- min(20, length(cooccurring))
  names(sort(cooccurring, decreasing = TRUE))[1:top_n]
}))

write_xlsx(top_twenty_cooccurring_words, "top_twenty_matrix_nsw.xlsx")


```

# ACCC Water Market's Inquiry scraping

```{r}
# scraping submissions to the issues paper

accc_wm_url <- "https://www.accc.gov.au/inquiries-and-consultations/finalised-inquiries/murray-darling-basin-water-markets-inquiry-2019-21/submissions-to-issues-paper"

# from the polite package, we properly identify ourselves and respect any explicit limits
session <- bow(accc_wm_url, force = TRUE)

# scrape the page contents
ACCC_page <- scrape(session)
```


```{r}
download_links <- tibble(link_names = ACCC_page %>%
                           html_nodes("a")%>%
                           html_text(),
                         link_urls = ACCC_page %>%
                           html_nodes("a") %>%
                           html_attr('href'))

download_links_docs <- download_links %>%
  filter(!str_detect(link_names, "PDF")) %>%
  filter(str_detect(link_urls, "system")) %>%
  mutate(link_names = str_trim(link_names, side = c("both"))) %>%
  mutate(link_urls = str_c("https://www.accc.gov.au", link_urls))

download_links_docs_subset <- download_links_docs %>%
  slice(c(1:10))
                           
```

```{r}
my_urls <- download_links_docs_subset$link_urls
save_here <- paste0("Issues paper submission -", download_links_docs$link_names, ".pdf")
mapply(download.file, my_urls, save_here, mode = "wb")

```


# ACCC Water Market's Inquiry scraping

```{r}
# scraping submissions to the interim report

accc_wm_url <- "https://www.accc.gov.au/inquiries-and-consultations/finalised-inquiries/murray-darling-basin-water-markets-inquiry-2019-21-0/submissions-to-interim-report"

# from the polite package, we properly identify ourselves and respect any explicit limits
session <- bow(accc_wm_url, force = TRUE)

# scrape the page contents
ACCC_page <- scrape(session)
```


```{r}
download_links <- tibble(link_names = ACCC_page %>%
                           html_nodes("a")%>%
                           html_text(),
                         link_urls = ACCC_page %>%
                           html_nodes("a") %>%
                           html_attr('href'))

download_links_docs <- download_links %>%
  filter(!str_detect(link_names, "PDF")) %>%
  filter(str_detect(link_urls, "system")) %>%
  mutate(link_names = str_trim(link_names, side = c("both"))) %>%
  mutate(link_urls = str_c("https://www.accc.gov.au", link_urls))

download_links_docs_subset <- download_links_docs %>%
  slice(c(1:10))
                           
```

```{r}
my_urls <- download_links_docs_subset$link_urls
save_here <- paste0("Interim report submission - ", download_links_docs$link_names, ".pdf")
mapply(download.file, my_urls, save_here, mode = "wb")

```


# Build corpus and create concordance

```{r}

files_accc_ir <- list.files(path = "docs_accc/interim_report/", pattern = "*.pdf", full.names = TRUE) 
submissions_accc_ir <- lapply(files_accc_ir, pdf_text)

submissions_accc_df <- submissions_accc_ir %>%  #create a dataframe to work with kwic
  map_df(~.x %>%  #ensuring that if there are any issues with different lengths (which there are) that these are left as NAs
           map(~if(length(.)) . else NA) %>% 
           do.call(what = cbind) %>% 
           as_tibble) %>%
  cbind(files_accc_ir) %>% #linking document names
  unite(submission, V2:V66, na.rm = TRUE) %>% #bringing text from multipage pdfs into a single column
  mutate(submission = str_squish(submission)) %>% # remove white space from beginning and end and double spaces to one space
  rowid_to_column("docname") %>% #create docname reference column
  mutate(docname = str_c("text", docname)) #create docname reference column

```


```{r}
submissions_accc_v <- unlist(submissions_accc) # turn documents into a vector for a single corpus

accc_sub_corpus <- corpus(submissions_accc_v) # turn vector into a corpus

accc_sub_corpus_df <- corpus( #corpus from dataframe (note syntax - to retain document boundaries
  submissions_accc_df,
  docid_field = "files_accc",
  text_field = "submission"
)

data_tokens_accc_sub <- tokens(accc_sub_corpus_df) #tokenize
kwic(data_tokens_accc_sub, pattern = "fair") # key words in context test

concordance_just_accc <- quanteda::kwic(
  data_tokens_accc_sub,
  pattern = "[a-z]*just[a-z]*",
  valuetype = "regex",
  window = 30) %>%
  as.data.frame() %>%
  filter(!str_detect(keyword, regex("adjust|justin", ignore_case = T)))

write_xlsx(concordance_just_accc, "concordance_just_accc.xlsx")

concordance_justice_accc <- quanteda::kwic(
  data_tokens_accc_sub,
  pattern = "[a-z]*justice[a-z]*",
  valuetype = "regex",
  window = 30) %>%
  as.data.frame() 

write_xlsx(concordance_justice_accc, "concordance_justice_accc.xlsx")

concordance_equit_accc <- quanteda::kwic(
  data_tokens_accc_sub,
  pattern = "[a-z]*equit[a-z]*",
  valuetype = "regex",
  window = 30) %>%
  as.data.frame() 

write_xlsx(concordance_equit_accc, "concordance_equit_accc.xlsx")

concordance_fair_accc <- quanteda::kwic(
  data_tokens_accc_sub,
  pattern = "[a-z]*fair[a-z]*",
  valuetype = "regex",
  window = 30) %>%
  as.data.frame() 

write_xlsx(concordance_fair_accc, "concordance_fair_accc.xlsx")

```

## Working with ACCC docs as a single document

```{r}

#Co-occurance matrix

accc_sub_sentences <- accc_sub_corpus%>%
  tolower() %>%
  paste0(collapse = " " ) %>%
  unlist() %>%
  tm::removePunctuation()%>%
  stringr::str_squish()

# Create a token object

accc_sub_token <- accc_sub_sentences %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_remove(stopwords('english')) 

# compute feature co-occurrence matrix (fcm)
fcm_accc_sub <- fcm(accc_sub_token, context = "window", window = 10)

# specify the words of interest
words_of_interest <- c("fair", "unfair", "equity", "inequity", "inequitable", "justice", "unjust", "injustice")

# filter the fcm to include only co-occurrences with the words of interest
fcm_filtered <- fcm_select(fcm_accc_sub, pattern = words_of_interest)

# print the filtered co-occurrence matrix
print(fcm_filtered)

# check if the words of interest are in the fcm_mat and remove unused words (stops 'subscript out of bounds' error)
words_of_interest <- words_of_interest[words_of_interest %in% rownames(fcm_accc_sub)]


#Words apart from the words of interest that co-occur
# get the rows of the fcm for the words of interest
fcm_interest <- fcm_accc_sub[words_of_interest, ]

# remove the columns corresponding to the words of interest from these rows
fcm_interest <- fcm_interest[, !(colnames(fcm_interest) %in% words_of_interest)]

# find the most common co-occurring words for each word of interest
top_cooccurring_words <- apply(fcm_interest, 1, function(row) {
    names(sort(row, decreasing = TRUE))
})

# find the top N co-occurring words for each word of interest
top_ten_cooccurring_words <- as.data.frame(apply(fcm_interest, 1, function(x) {
  cooccurring <- x[!(names(x) %in% words_of_interest)]
  top_n <- min(10, length(cooccurring))
  names(sort(cooccurring, decreasing = TRUE))[1:top_n]
}))

write_xlsx(top_ten_cooccurring_words, "top_ten_matrix_accc.xlsx")

top_twenty_cooccurring_words <- as.data.frame(apply(fcm_interest, 1, function(x) {
  cooccurring <- x[!(names(x) %in% words_of_interest)]
  top_n <- min(20, length(cooccurring))
  names(sort(cooccurring, decreasing = TRUE))[1:top_n]
}))

write_xlsx(top_twenty_cooccurring_words, "top_twenty_matrix_accc.xlsx")


```

# Productivity Commission

```{r}
pc_nwr_url <- "https://www.pc.gov.au/inquiries/completed/water-reform-2020/submissions"

# from the polite package, we properly identify ourselves and respect any explicit limits
# session <- bow(pc_nwr_url, force = TRUE)

# scrape the page contents
#pc_page <- scrape(session)

pc_page <- read_html(pc_nwr_url)
```

```{r}
download_links_pc <- tibble(link_names = pc_page %>%
                           html_nodes("a")%>%
                           html_text(),
                         link_urls = pc_page %>%
                           html_nodes("a") %>%
                           html_attr('href'))

# add a column to allow separation off the submissions into pre and post submissions to put in different folders

pc_submission_type <- c("#post-draft", "#initial", "#brief")

download_links_pc <- download_links_pc %>%
  mutate(submission_type = if_else(link_urls %in% pc_submission_type, link_urls, NA_character_)) %>%
  fill(submission_type) %>%
  mutate(submission_type = str_replace(submission_type, "#", ""))

# filter for just pdfs and change attachment names so that they include the name of the head submission and remove extraneous "/" characters

download_links_docs_pc <- download_links_pc %>%
  filter(str_detect(link_names, "PDF")) %>%
  mutate(link_names = str_replace(link_names, "\\s*\\(PDF\\s*-.*\\)$", "")) %>%
  mutate(
    current_name = ifelse(!str_detect(link_names, "^Attachment"), link_names, NA)
  ) %>%
  fill(current_name) %>%
  mutate(
    link_names = ifelse(str_detect(link_names, "^Attachment"), paste(current_name, link_names), link_names)
  ) %>%
  select(-current_name) %>%
  mutate(link_names = str_replace(link_names, "//", ""))



```

```{r}
# Function to download the PDFs and save in a defined directory

#pc_subdirectories <- c("productivity_commission_submissions/initial", "productivity_commission_submissions/post-draft", "productivity_commission_submissions/brief")
#walk(pc_subdirectories, dir.create, recursive = TRUE)

pc_subdirectory <- "productivity_commission_submissions_NWR_2020"

download_pdf <- function(link_urls, link_names, submission_type) {
  # create subdirectories based on submission type
  pc_paths <- file.path(pc_subdirectory, submission_type)
  dir.create(pc_paths, recursive = TRUE, showWarnings = FALSE)
  # add .pdf extension to link_names
  link_names <- paste0(link_names, ".pdf")
  # save the PDFs in the respective subdirectory
  save_path_pc <- file.path(pc_paths, link_names)
  httr::GET(link_urls, httr::write_disk(path = save_path_pc, overwrite = TRUE))
}

# Apply the function to each row of the data frame
pmap(download_links_docs_pc, download_pdf)
##

```

```{r}

# Import the PDFs and build initial corpus

directories <- c("productivity_commission_submissions_NWR_2020/brief",
                 "productivity_commission_submissions_NWR_2020/initial",
                 "productivity_commission_submissions_NWR_2020/post-draft")

# Function to list and read PDFs
read_pdfs_in_dir <- function(dir) {
  list.files(path = dir, pattern = "*.pdf", full.names = TRUE) %>% 
    map_df(~{
      tryCatch({
        text <- pdf_text(.x)
        clean_text <- gsub("[^[:print:]]", "", text)
        clean_text_collapsed <- paste(clean_text, collapse = " ")
        tibble(filename = .x, text = clean_text_collapsed)
      }, error = function(e) {
        message(paste("Error processing file:", .x))
        tibble(filename = .x, text = NA_character_)
      })
    })
}


pc_submissions_text <- map_df(directories, read_pdfs_in_dir)

```

```{r}

# Create one big concordance

# Regular expressions for key words
pc_efj <- c("fair\\w*", "unfair\\w*", "equit\\w*", "inequit\\w*", "just\\w*", "unjust\\w*", "injust\\w*")

# Pattern string for use in regex
pattern_string <- paste(pc_efj, collapse = "|")

pc_concordance <- mdba_sub_text_data %>%
  corpus()%>%
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>%
  kwic(pattern = pattern_string, valuetype = "regex", window = 30, case_insensitive = TRUE)

```


```{r}
# The single concordance is too big - break it down to individual words as in the above, but write more efficiently than the previous code and save automatically into a defined path

# create subdirectory
pc_concord_subdirectory <- "productivity_commission_submissions_NWR_2020/pc_concordance"

dir.create(pc_concord_subdirectory, recursive = T, showWarnings = F)


# Define key word patterns
pc_efj <- c("\\bfair\\w*", "\\bunfair\\w*", "\\bequit\\w*", "\\binequit\\w*", "\\bjust\\w*", "unjust\\w*", "injust\\w*")

# Tokenise the mdba texts and create a corpus
pc_sub_tokens <- corpus(pc_submissions_text$text, docnames = pc_submissions_text$filename) %>%
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)

# Iterate over the keyword patterns and export as excel files
map(pc_efj, ~{
  # Generate KWIC concordance for the current pattern
  kwic_concordance <- pc_sub_tokens %>%
    kwic(pattern = .x, valuetype = "regex", window = 30, case_insensitive = TRUE) %>%
    as.data.frame()

  # Create a filename for the Excel file
  file_name <- paste0(pc_concord_subdirectory, "/", "pc_concordance_", gsub("\\\\w\\*", "", .x), ".xlsx")

  # Export the KWIC concordance to Excel
  write_xlsx(kwic_concordance, path = file_name)
})
```

## Alternative approach that allows for creation of a dataframe to count documents using the terms and usage within documents.

```{r}

# Define key word patterns
pc_efj <- c("\\bfair\\w*", "\\bunfair\\w*", "\\bequit\\w*", "\\binequit\\w*", 
            "\\bjust\\w*", "unjust\\w*", "injust\\w*")

# Tokenise the pc submissions and create a corpus
pc_sub_tokens <- corpus(pc_submissions_text$text, docnames = pc_submissions_text$filename) %>%
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)

# Iterate over the keyword patterns and export as excel files, and collect data into one dataframe
all_data <- map_dfr(pc_efj, ~{
  # Generate KWIC concordance for the current pattern
  kwic_concordance <- pc_sub_tokens %>%
    kwic(pattern = .x, valuetype = "regex", window = 30, case_insensitive = TRUE) %>%
    as.data.frame()

  # Create a filename for the Excel file
  file_name <- paste0(pc_concord_subdirectory, "/", "pc_concordance_", gsub("\\\\w\\*", "", .x), ".xlsx")

  # Export the KWIC concordance to Excel
  write_xlsx(kwic_concordance, path = file_name)
  
  # Return the data
  tibble(filename = kwic_concordance$docname,
         keyword = .x,
         token = tolower(kwic_concordance$keyword))
})

# Count the number of documents that contain each string pattern
doc_counts <- all_data %>%
  group_by(token) %>%
  summarise(num_docs = n_distinct(filename), 
            proportion = num_docs / n_distinct(pc_submissions_text$filename))

# Count the number of times each string pattern appears in each document
token_counts <- all_data %>%
  count(filename, token)

```

# Topic modelling with the productivity commission submissions

```{r}
# Continue preprocessing

pc_sub_dfm <- pc_sub_tokens %>%
  tokens_tolower %>% #change to lower case
  tokens_remove(stopwords("english")) %>% # remove English stop words
  dfm() %>% # change to a document feature matrix
  dfm_trim(min_count = 5) # trim infrequent word to reduce computational time in topic modelling

#pc_lda_model <- textmodel_lda(dfm, k =k) # doesn't appear to work with quanteda approach LDA not available need to change dfm to a dtm and use the topicmodels package

pc_sub_dtm <- convert(pc_sub_dfm, to = "topicmodels")
```

## Using SpaCy to lemmatise text 

```{r}
# Set up the python environment

library(reticulate)
use_condaenv("base", required = TRUE)

# Start Spacy in R
spacy_initialize(model = 'en_core_web_sm')
```



```{r}
# Convert tokens to a list
pc_sub_list <- as.list(pc_sub_tokens)

# Concatenate the tokens in each document
pc_sub_texts <- pc_sub_list %>%
  map_chr(paste, collapse = " ")

# Parse the texts
pc_sub_texts_parsed <- sapply(pc_sub_texts, function(x) {
  parsed_text <- spacy_parse(x)
  parsed_text$lemma %>%
    paste(collapse = " ")
})

# Create pattern to match any punctuation
pattern_punct <- "[^[:alnum:][:space:]]+"

# Tokenize the parsed texts
pc_sub_tokens_lemmatized <- tokens(pc_sub_texts_parsed, what = 'word') %>%
  tokens_remove(pattern_punct)

# Create a dtm of lemmatised tokens

pc_sub_tokens_lemmatized_dfm <- dfm(pc_sub_tokens_lemmatized)

pc_sub_tokens_lemmatized_dtm <- convert(pc_sub_tokens_lemmatized_dfm, to = "topicmodel")


```


## Topic modelling

```{r}
# Topic modelling setting K

pc_k <- 40 # number of topics


pc_lda_model <- LDA(pc_sub_dtm, k = pc_k, method = "Gibbs", control = list(seed = 1234))

#view terms

N <- 20
terms(pc_lda_model, N)

```

## Topic modelling with lemmatised text

```{r}

pc_k <- 20 # number of topics


pc_lda_model_lemma <- LDA(pc_sub_tokens_lemmatized_dtm, k = pc_k, method = "Gibbs", control = list(seed = 1234))

#view terms

N <- 20
pc_lda_model_20_top_terms_lemma_df <- as.data.frame(terms(pc_lda_model_lemma, N))

```


```{r}

```



```{r}
# Identify "equity" submissions - 

equity_keywords <-  c("\\bfair\\w*", "\\bunfair\\w*", "\\bequit\\w*", "\\binequit\\w*", "\\bjustice", "unjust\\w*", "injust\\w*")

# identify which terms match these patterns

matching_terms <- equity_keywords %>% 
  map(~grep(.x, Terms(pc_sub_dtm), value = TRUE)) %>%
  unlist() %>%
  unique()

#clean up the list

terms_remove <- c("https\\w*", "fairy\\w*", "fairb\\w*", "fairl\\w*")

#match terms to patterns in matching-terms

terms_remove <- unique(unlist(map(terms_remove, ~grep(.x, matching_terms, value = TRUE))))

# Remove the terms from matching_terms
matching_terms <- setdiff(matching_terms, terms_remove)

#identify "equity" submissions
pc_equity_submissions <- slam::row_sums(pc_sub_dtm[, matching_terms]) > 0

# Create a matrix of topic distributions for each document

pc_topic_distributions <- posterior(pc_lda_model)$topics

# Check the topic distribution for each document sums to one

pc_topic_distro_row_sums <- rowSums(pc_topic_distributions)

# Check if the row sums are approximately 1 (within a small tolerance)
all(abs(pc_topic_distro_row_sums - 1) < 1e-9)

```

# Inspecting the topic modelled distributions

## Heat map

```{r}
# Convert the pc_topic_distributions matrix into a data frame
pc_topic_distributions_df <- as.data.frame(pc_topic_distributions)

# Make sure both pc_topic_distributions and pc_equity_submissions are in the same order
pc_equity_submissions <- pc_equity_submissions[rownames(pc_topic_distributions_df)]

# Create a custom color scaling function - logs needed to see differences 
log_breaks <- function(n) {
  exp(seq(log(min(topic_distributions_df)), log(max(topic_distributions_df)), length.out = n))
}
my_palette <- colorRampPalette(viridis(256))

# Create the heatmap with a logarithmic color scale
heatmaply(pc_topic_distributions_df, 
          breaks = log_breaks,
          colors = my_palette,
          Rowv = NULL, # Disable row clustering
          Colv = NULL, # Disable column clustering
          showticklabels = c(FALSE, FALSE), # Hide tick labels for a cleaner look
          row_side_colors = pc_equity_submissions, # Use the pc_equity_submissions to color the row side bar
          row_side_palette = c("TRUE" = "black", "FALSE" = "white")) # Color equity submissions black and others white

```
## PCA biplot

A PCA biplot provides a two-dimensional summary of the variation in the topic model, showing both how the topics (represented as vectors) relate to each other and how the documents (represented as points) relate to the topics.

The vectors in the biplot represent the topics, with the direction and length of a vector indicating how that topic contributes to the two principal components. The points in the biplot represent the documents, with the position of a point indicating how the corresponding document's topic distribution relates to the two principal components.

The principle components are represented by the arrows or vectors that indicate the direction and magnitude of each component. These arrows represent the loading vectors, which show the contribution of each variable (or in this case, each topic) to the components.

In the context of the PCA biplot above, the principle components are the axes along which the data points are projected. The length and direction of the arrows indicate the strength and direction of the relationship between each topic and the principle components.

Typically, the biplot displays the first two principle components, PC1 and PC2, in a two-dimensional space. PC1 represents the direction that captures the maximum variance in the data, while PC2 represents the next largest orthogonal direction. These components are ordered based on the amount of variation they explain in the data.

To interpret the biplot, examine the angles and distances between the arrows and the data points. The closer the data points are to a particular arrow, the higher the correlation between that variable (topic) and the corresponding principle component. The angle between two arrows indicates the correlation or relationship between the topics they represent.


```{r}
# Load necessary libraries
library(ggplot2)
library(ggfortify)

# Conduct PCA on the topic distributions
pc_pca_result <- prcomp(pc_topic_distributions)

# Convert the equity submissions vector into a data frame
pc_equity_submissions_df <- as.data.frame(pc_equity_submissions)

# Generate a PCA biplot, color-coding the points by equity submission status
autoplot(pc_pca_result, data = pc_equity_submissions_df, colour = 'pc_equity_submissions', 
         loadings = TRUE, loadings.colour = 'blue', loadings.label = TRUE, loadings.label.size = 3)
```
## UMAP scatter plot

UMAP (Uniform Manifold Approximation and Projection) plot reduces the high-dimensional topic distribution of each document to a 2D scatter plot. Each document is a point and documents with similar topics should be near each other. 

```{r}
library(umap)

# UMAP model
umap_model <- umap::umap(pc_topic_distributions)

# Store the embeddings as a data frame
umap_data <- data.frame(UMAP1 = umap_model$layout[,1], 
                        UMAP2 = umap_model$layout[,2],
                        Equity = as.factor(pc_equity_submissions))

# Plot the UMAP embeddings
ggplot(umap_data, aes(x = UMAP1, y = UMAP2, color = Equity)) +
  geom_point() +
  scale_color_manual(values = c("red", "blue")) + 
  labs(title = "UMAP projection of the LDA topic model",
       x = "UMAP1",
       y = "UMAP2",
       color = "Equity submission") +
  theme_minimal()

```
Let's make the umap plot interactive using plotly

```{r}
library(plotly)

# Add document names to umap_data
umap_data$Document = rownames(pc_topic_distributions)

# Create plotly interactive plot
pc_topic_model_plot <- plot_ly(umap_data, 
                x = ~UMAP1, 
                y = ~UMAP2, 
                color = ~Equity, 
                colors = c("red", "blue"),
                hovertext = ~Document,
                hoverinfo = "text",
                type = "scatter",
                mode = "markers") 

pc_topic_model_plot <- layout(pc_topic_model_plot, 
               title = "UMAP projection of the LDA topic model",
               xaxis = list(title = "UMAP1"),
               yaxis = list(title = "UMAP2"),
               legend = list(title = list(text = '<b> Equity submission </b>')))

pc_topic_model_plot

```
There are some clusters here interacting with the documents. Let's try K-means clustering and see what happens:

```{r}
# K-means clustering
set.seed(42)  # for reproducibility
k <- 5  # choose the number of clusters
pc_topic_kmeans_model <- kmeans(pc_topic_distributions, centers = k)

# Add the cluster assignments to umap_data
umap_data$Cluster <- as.factor(pc_topic_kmeans_model$cluster)

# Convert the Equity column to logical type
umap_data$Equity <- as.logical(umap_data$Equity)

# Create a new column for the hover text
umap_data$hover_text <- paste("Submission: ", row.names(umap_data),
                              "<br>Cluster: ", umap_data$Cluster,
                              "<br>Equity Submission: ", umap_data$Equity)

plot_ly(umap_data, 
        x = ~UMAP1, 
        y = ~UMAP2, 
        color = ~Cluster, 
        type = "scatter", 
        mode = "markers", 
        text = ~hover_text, 
        hoverinfo = "text", 
        marker = list(symbol = ~ifelse(Equity, "circle", "square"))) %>%
  layout(title = "UMAP projection with K-means clustering")

```
```{r}
# updated plotly plot - show equity and non equity

# Create separate equity and non-equity datasets
umap_data_equity <- umap_data[umap_data$Equity,]
umap_data_non_equity <- umap_data[!umap_data$Equity,]

# Create separate plots for equity and non-equity data
pc_topic_model_plot_ctrl <- plot_ly(
    umap_data_equity, 
    x = ~UMAP1, 
    y = ~UMAP2, 
    color = ~as.factor(Cluster), 
    type = "scatter", 
    mode = "markers", 
    text = ~hover_text, 
    hoverinfo = "text", 
    marker = list(symbol = "circle"),
    name = "Equity"
)

pc_topic_model_plot_ctrl <- add_trace(
    pc_topic_model_plot_ctrl,
    x = ~UMAP1, 
    y = ~UMAP2, 
    color = ~as.factor(Cluster), 
    type = "scatter", 
    mode = "markers", 
    text = ~hover_text, 
    hoverinfo = "text", 
    data = umap_data_non_equity, 
    marker = list(symbol = "square"),
    name = "Non-Equity"
)

# Display the plot
pc_topic_model_plot_ctrl

```

## Hierarchical clustering

```{r}
# Compute the Euclidean distance matrix
dist_matrix <- dist(pc_topic_distributions_df)

# Perform hierarchical clustering
hc <- hclust(dist_matrix)

# Cut the dendrogram to get 5 clusters
clusters <- cutree(hc, k = 5)

# Add the cluster assignments to your data
pc_topic_distributions_df$Cluster <- as.factor(clusters)

# Compute the dominant topic in each document
pc_topic_distributions_df$Dominant_Topic <- apply(pc_topic_distributions_df[,1:(ncol(pc_topic_distributions_df)-1)], 1, which.max)

# Create a dendrogram
dendro <- as.dendrogram(hc)

# Plot the dendrogram
plot(dendro)

# Color branches by cluster assignment
dendro_colored <- color_branches(dendro, k = 5)
plot(dendro_colored)

```

## Running the LDA model with fewer topics - test for robustness

```{r}
# Topic modelling setting K

pc_k_20 <- 20 # number of topics
#pc_lda_model <- textmodel_lda(dfm, k =k) # doesn't appear to work with quanteda approach LDA not available

pc_lda_model_20 <- LDA(pc_sub_dtm, k = pc_k_20, method = "Gibbs", control = list(seed = 1234))

#view terms

N <- 20
pc_lda_model_20_top_terms_df <- as.data.frame(terms(pc_lda_model_20, N))

pc_lda_model_terms_output_table_20 <- pc_lda_model_20_top_terms_df %>%
  kable("html", caption = "Top Terms per Topic") %>%
  kable_styling(bootstrap_options = c("striped", "condensed"))

pc_lda_model_terms_output_table_20

# See the table_report.R script and rmd docs for exporting this kableExtra table in the folder for this project.


```

```{r}
# Topic modelling setting K at various levels

library(kableExtra)
library(rmarkdown)

# Possible numbers of topics to test
test_k <- seq(5, 15, by = 5)
names(test_k) <- as.character(test_k)
# Find the models with different numbers of topics using LDA
pc_topic_models <- test_k %>% map(~ LDA(pc_sub_dtm, k = .x, method = "Gibbs", control = list(seed = 1234)))

#pc_topic_models <- test_k %>% map(~ LDA(pc_sub_dtm, k = .x, control = list(seed = 1234))) # this defaults to  Variational Expectation-Maximization (VEM), a faster, but less accurate approach - noticably less accurate see examples in output folder

# Create a list of data frames from topic models 

pc_topic_data_frames <- pc_topic_models %>% map(~ as.data.frame(terms(.x, 20)))

# Create new subdirectory

dir.create("productivity_commission_submissions_NWR_2020/topic_models")

# Create HTML files for each topic model

# Create HTML files for each topic model
# Create HTML files for each topic model
pc_topic_data_frames %>% 
  map2(names(test_k), function(df, i) {
    # Create a temporary .Rmd file in the desired directory
    rmd_file <- tempfile(tmpdir = "productivity_commission_submissions_NWR_2020/topic_models", fileext = ".Rmd")
    
    # Write an .Rmd file with inline R code
    writeLines(
      c("---",
        "title: 'Productivity Commission 2020 National Water Reform Inquiry - Submission topic model'",
        "output: html_document",
        "---",
        "",
        "`r kableExtra::kable_styling(knitr::kable(df, 'html'))`"),
      rmd_file)
    
    # Render the .Rmd file to HTML
    rmarkdown::render(rmd_file, output_file = paste0("Topic_Model_", i, "_topics.html"))

    # Delete the temporary .Rmd file
    file.remove(rmd_file)
  })



```


```{r}
# calculate topic distributinos 
pc_topic_distributions_20 <- posterior(pc_lda_model_20)$topics
```


```{r}
# UMAP model
umap_model_20 <- umap::umap(pc_topic_distributions_20)

# Store the embeddings as a data frame
umap_data_20 <- data.frame(UMAP1 = umap_model_20$layout[,1], 
                        UMAP2 = umap_model_20$layout[,2],
                        Equity = as.factor(pc_equity_submissions))

# Add document names to umap_data
umap_data$Document = rownames(pc_topic_distributions)
```


```{r}
# K-means clustering
set.seed(42)  # for reproducibility
k <- 5  # choose the number of clusters
pc_topic_kmeans_model_20 <- kmeans(pc_topic_distributions_20, centers = k)

# Add the cluster assignments to umap_data
umap_data_20$Cluster <- as.factor(pc_topic_kmeans_model_20$cluster)

# Convert the Equity column to logical type
umap_data_20$Equity <- as.logical(umap_data_20$Equity)
```


```{r}
# Create a new column for the hover text
umap_data_20$hover_text <- paste("Submission: ", row.names(umap_data),
                              "<br>Cluster: ", umap_data$Cluster,
                              "<br>Equity Submission: ", umap_data$Equity)

# Create separate equity and non-equity datasets
umap_data_equity_20 <- umap_data_20[umap_data_20$Equity,]
umap_data_non_equity_20 <- umap_data_20[!umap_data_20$Equity,]



# Create separate plots for equity and non-equity data
pc_topic_model_plot_ctrl_20 <- plot_ly(
    umap_data_equity_20, 
    x = ~UMAP1, 
    y = ~UMAP2, 
    color = ~as.factor(Cluster), 
    type = "scatter", 
    mode = "markers", 
    text = ~hover_text, 
    hoverinfo = "text", 
    marker = list(symbol = "circle"),
    name = "Equity"
)

pc_topic_model_plot_ctrl_20 <- add_trace(
    pc_topic_model_plot_ctrl_20,
    x = ~UMAP1, 
    y = ~UMAP2, 
    color = ~as.factor(Cluster), 
    type = "scatter", 
    mode = "markers", 
    text = ~hover_text, 
    hoverinfo = "text", 
    data = umap_data_non_equity_20, 
    marker = list(symbol = "square"),
    name = "Non-Equity"
)

# Display the plot
pc_topic_model_plot_ctrl_20

htmlwidgets::saveWidget(pc_topic_model_plot_ctrl_20, "pc_topic_model_plot_ctrl_20.html")

```


```{r}
# Convert the pc_topic_distributions matrix into a data frame
pc_topic_distributions_20_df <- as.data.frame(pc_topic_distributions_20)

#order the pc_equity_submssions vector so equity submissions come first

pc_equity_order_vector_sorted <- order(pc_equity_submissions, decreasing = TRUE)

# Apply the order to the dataframe and equity submissions
pc_topic_distributions_20_df <- pc_topic_distributions_20_df[pc_equity_order_vector_sorted, ]
pc_equity_submissions <- pc_equity_submissions[pc_equity_order_vector_sorted]

# Check order is the same
#print(names(pc_equity_submissions))
#print(rownames(pc_topic_distributions_20_df))

# Create a custom color scaling function - logs needed to see differences 
log_breaks <- function(n) {
  exp(seq(log(min(topic_distributions_20_df)), log(max(topic_distributions_20_df)), length.out = n))
}
my_palette <- colorRampPalette(c("white", "steelblue"))(100) #colorRampPalette(viridis(256))


# Convert the logical vector to a factor with specified level order
pc_equity_submissions_factor <- factor(pc_equity_submissions, levels = c(TRUE, FALSE))

# Create the heatmap with a logarithmic color scale
pc_topicmodel_heatmap <- heatmaply(pc_topic_distributions_20_df, 
          breaks = log_breaks,
          colors = my_palette,
          Rowv = NULL, # Use the custom dendrogram 
          Colv = NULL, # Disable column clustering
          showticklabels = c(FALSE, FALSE), # Hide tick labels for a cleaner look
          row_side_colors = as.character(pc_equity_submissions_factor), # Use the pc_equity_submissions to color the row side bar - note that the dentrogram is driving the order, so you can't cluster the equity submissions together
          row_side_palette = c("TRUE" = "black", "FALSE" = "white")) # Color equity submissions black and others white

#View the plot
pc_topicmodel_heatmap

#export the plot
htmlwidgets::saveWidget(pc_topicmodel_heatmap, "pc_topic_model_heatmap.html")
```

# Testing for an "optimal" number of topics 

## Coherence scores

Coherence score captures semantic similarity between high scoring words in the topic. A higher coherence score indicates that the topics are more human-interpretable.

```{r}
# Load necessary packages
library(ldatuning)
library(SnowballC)

# Possible numbers of topics to test - seems like this is too computationally intensive for my desktop - I halted the approach after 10 minutes waiting - Sam advises that this is probably best done manually and qualitatively 

test_k <- seq(5, 20, by = 5) 

# Find the models with different numbers of topics using LDA
topic_models <- lapply(test_k, function(i) LDA(pc_sub_dtm, k = i, control = list(seed = 1234)))

# Calculate the Grün and Hornik (2011) topic coherence
gh_coherence <- sapply(topic_models, GH_coherence, pc_sub_dtm = pc_sub_dtm)

# Calculate the Mimno et al. (2011) topic coherence
mimno_coherence <- sapply(topic_models, mimno_coherence, pc_sub_dtm = pc_sub_dtm)

# Calculate the Newman et al. (2010) topic coherence
newman_coherence <- sapply(topic_models, newman_coherence, pc_sub_dtm = pc_sub_dtm, measure = "UMass")

# Calculate the Röder et al. (2015) topic coherence
roeder_coherence <- sapply(topic_models, roeder_coherence, pc_sub_dtm = pc_sub_dtm, measure = "c_v")

# Plot number of topics against coherence measures
plot(test_k, gh_coherence, type = "b", xlab = "Number of topics", ylab = "Coherence measure",
     main = "Optimal number of topics", col = "red", lwd = 2)

lines(test_k, mimno_coherence, type = "b", col = "green", lwd = 2)
lines(test_k, newman_coherence, type = "b", col = "blue", lwd = 2)
lines(test_k, roeder_coherence, type = "b", col = "purple", lwd = 2)

legend("topleft", legend = c("GH", "Mimno", "Newman_UMass", "Roeder_c_v"), col = c("red", "green", "blue", "purple"), lty = 1, lwd = 2)

```

## Alternative topic modelling approach using Online Variational Bayes

Variational methods are generally faster than Gibbs sampling (which is used by LDA() in the topicmodels package), although they may not perform as well on some metrics. The text2vec package in R provides a function for online variational Bayes LDA.

```{r}
library(text2vec)

OVB_pc_lda_model = LDA$new(n_topics = 10, doc_topic_prior = 0.1, topic_word_prior = 0.01)
doc_topic_distr = OVB_pc_lda_model$fit_transform(x = pc_sub_dtm, n_iter = 1000, convergence_tol = 0.001, n_check_convergence = 25, progressbar = FALSE)


```


# Calcualting similarity measures

## Jaccard and cosine similarity

```{r}

```



## Jensen-Shannon Divergence

```{r}
# Calculate the Jensen-Shannon Divergence for each pair of documents

# Create a dataframe of all combinations of document indices
pc_topic_distro_doc_pairs <- expand.grid(i = 1:nrow(pc_topic_distributions), j = 1:nrow(pc_topic_distributions))

# Function to compute JSD between a pair of documents
pc_topic_distro_compute_jsd <- function(i, j) {
   distributions <- rbind(pc_topic_distributions[i, ], pc_topic_distributions[j, ])
  # compute JSD
  JSD(distributions, unit = "log2")
}

# Use pmap to apply compute_jsd to each pair of documents
pc_topic_distro_jsd_values <- pmap_dbl(pc_topic_distro_doc_pairs, pc_topic_distro_compute_jsd)

# Convert the result to a matrix
pc_topic_distro_jsd_matrix <- matrix(pc_topic_distro_jsd_values, nrow = nrow(pc_topic_distributions), byrow = TRUE)

```




```{r}
# Co-occurance matrix 

mdba_nb_sub_sentences <- mdba_sub_text_data$text %>%
  tolower() %>%
  paste0(collapse = "") %>%
  unlist() %>%
  tm::removePunctuation()%>%
  stringr::str_squish()

# Create a token object

mdba_nb_sub_token <- mdba_nb_sub_sentences %>%
  tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_remove(stopwords('english')) 

# Compute feature co-occurance matrix

fcm_mdba_nb_sub <- fcm(mdba_nb_sub_token, context = "window", window = 10)

# specify the words of interest
words_of_interest_mdba <- c("fair", "unfair", "equity", "inequity", "inequitable", "justice", "unjust", "injustice")

# check if the words of interest are in the fcm_mat and remove unused words (stops 'subscript out of bounds' error)
words_of_interest_mdba <- words_of_interest_mdba[words_of_interest_mdba %in% rownames(fcm_mdba_nb_sub)]

# Strange output for some words when calculating co-occurance matrices - suspect this is because they occur infrequently  - let's try a frequency list and remove all works with a frequency of less than 10

# Create a frequency list of the tokens
token_freqs_mdba <- table(unlist(tokens(mdba_nb_sub_token)))

# Subset to check the counts for the words of interest
words_of_interest_counts_mdba <- token_freqs_mdba[names(token_freqs_mdba) %in% words_of_interest_mdba]

words_of_interest_mdba <- names(words_of_interest_counts_mdba)[words_of_interest_counts_mdba >= 10]


#Words apart from the words of interest that co-occur
# get the rows of the fcm for the words of interest
fcm_interest_mdba <- fcm_mdba_nb_sub[words_of_interest_mdba, ]

# remove the columns corresponding to the words of interest from these rows
fcm_interest_mdba <- fcm_interest_mdba[, !(colnames(fcm_interest_mdba) %in% words_of_interest)]

# find the most common co-occurring words for each word of interest
top_cooccurring_words_mdba <- apply(fcm_interest_mdba, 1, function(row) {
    names(sort(row, decreasing = TRUE))
})

# find the top N co-occurring words for each word of interest
top_ten_cooccurring_words_mdba <- as.data.frame(apply(fcm_interest_mdba, 1, function(x) {
  cooccurring <- x[!(names(x) %in% words_of_interest_mdba)]
  top_n <- min(10, length(cooccurring))
  names(sort(cooccurring, decreasing = TRUE))[1:top_n]
}))

top_twenty_cooccurring_words_mdba <- as.data.frame(apply(fcm_interest_mdba, 1, function(x) {
  cooccurring <- x[!(names(x) %in% words_of_interest_mdba)]
  top_n <- min(20, length(cooccurring))
  names(sort(cooccurring, decreasing = TRUE))[1:top_n]
}))


# Looking at all toekns in the fcm calculate token frequencies, sort, and convert to data frame

all_feature_counts_df_mdba <- dfm(mdba_nb_sub_token) %>%
  colSums() %>%
  sort(decreasing = TRUE) %>%
  as.data.frame() %>%
  rownames_to_column(var = "word") %>%
  rename(frequency = ".")

# Or just for the top ten

all_feature_counts_df_top_ten <- dfm(mdba_nb_sub_token)  %>% 
  topfeatures() %>% 
  as_tibble(rownames = "token") %>% 
  rename(frequency = value)

# Chart the top 50

all_feature_counts_df_mdba %>%
  head(50) %>%
  ggplot(aes(x = reorder(word, frequency), y = frequency)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(x = "Word", y = "Frequency", title = "Top 50 Word Frequencies") +
  theme_minimal() +
  theme(axis.text = element_text(size = 6))

dir.create("docs_mdba/bp_amendments_other analysis", recursive = T, showWarnings = F)

mdba_output_list <- list(all_feature_counts_df_mdba =all_feature_counts_df_mdba, all_feature_counts_df_top_ten = all_feature_counts_df_top_ten)

output_dir <- "docs_mdba/bp_amendments_other analysis/"

mdba_output_list %>%
      imap(function(df, name) {
    filename <- paste0(output_dir, name, ".xlsx")
    write_xlsx(df, filename)
  })




```


# Further work from here down still underway - not all code working...

```{r}
# Define co-occurrence function NOT YET WORKING - GO BACK TO PREVIOUS APPROACH
co_occurrence <- function(dfm, keywords, N){
  cooccur_mat <- fcm(dfm, context = "document", count = "frequency", tri = FALSE)
  cooccur_df <- as.data.frame(as.matrix(cooccur_mat))
  
  top_cooccur <- lapply(keywords, function(keyword) {
    if(keyword %in% names(cooccur_df)) {
      keyword_cooccur <- cooccur_df[,keyword]
      keyword_cooccur <- keyword_cooccur[order(-keyword_cooccur)][1:N]
    } else {
      keyword_cooccur <- NULL
    }
    return(keyword_cooccur)
  })
  
  names(top_cooccur) <- keywords
  return(top_cooccur)
}

# Keywords
keywords <- c("fairness", "equity", "justice")

# Create co-occurrence matrix
top_cooccur <- co_occurrence(mdba_sub_dfm, keywords, 10)

# View results
print(top_cooccur)

```



