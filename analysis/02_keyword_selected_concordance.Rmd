# Keyword, Concordance and Collocation Analysis

This analysis aims to build keyword lists ranking words that are overused in
particular inquiries, and by particular types of inquiry submissions. This is
the starting point for further more detailed analysis of the individual 
documents and to better understand which words are key for understanding the
different landscapes of submissions (and submitters).

The statistical procedures here are inspired by standard keyword detection as
used in corpus linguistics, but with some key differences:

1. The unit of analysis is the "submission", not the words used in a submission
unlike corpus linguistics. This is important because there is significant 
variation in basic things like the size of submissions - using individual words
as the unit of analysis would put more weight on the longest submissions.

```{r eval=FALSE}
install.packages("quanteda")
install.packages("quanteda.textstats")
install.packages("RSQLite")
install.packages("writexl")
install.packages("tidyverse")
install.packages("xtable")
```

# Load and prepare are submission data as extracted from the various inquiries
```{r}
library(DBI)
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
library(writexl)
library(xtable)

inquiries <- dbConnect(RSQLite::SQLite(), "../inquiries/inquiries.db")
submissions <- dbGetQuery(inquiries, 
  'SELECT submission.*, submission_label.label, context FROM submission
  inner join submission_label using(inquiry_shortname, submission_id)
  inner join inquiry using(inquiry_shortname)
  where priority = 1
  '
) %>% mutate(text = gsub("[‘’]", "'", text))

# Generate a single column unique id - this is the composite primary key
# defined in the database, so will always be unique. This is used to tie
# together results at different granularities.
submissions$doc_id <- paste(submissions$inquiry_shortname, submissions$submission_id)
submission_corpus <- corpus(submissions, text_field="text", docid_field = "doc_id")

dbDisconnect(inquiries)

```

# Initial Keyword Ranking

We'll start by assembling ranked keyword lists for each stakeholder group.

This will take the approach of computing the chi-squared measure for differences
for each group vs all other groups aggregated together. Unlike conventional
corpus linguistics we will focus on word presence or absence in a document:
if we took the conventional corpus linguistics approach and counted words
aggregated across all stakeholder groups we would place significantly more
weight on the longest submissions. 

```{r}

# These are the byproducts of extracting text from the PDF where images are
# included.
pdf_processing_words <- c(
  'image',
  'height',
  'width',
  'rgb',
  'srgb',
  'bpc',
  '16',
  'iccbased'
)
# These words are potential keywords, but reflect the genre and form of the
# submission, not the substantive content.
genre_words <- c(
  'pty', 
  'ltd',
"etc",
"re",
"amendment",
"bill",
"chamber",
"coalition",
"commend",
"debate",
"deputy",
"election",
"electorate",
"greens",
"hanson",
"labor",
"legislation",
"liberal",
"minister",
"opposition",
"prime",
"rise",
"said",
"say",
"saying",
"senator",
"something",
"speak",
"speaker",
"speech",
"sure",
"talk",
"talked",
"talking",
"things",
"think",
"today",
"want",
"went",
"authority",
"authority",
"basin",
"darling",
"sincerely",
"submission",
"067",
"1801",
"bag",
"compiling",
"contact",
"cr",
"fax",
"gpo",
"hon",
"manager",
"mayor",
"mlc",
"mp",
"nicholson",
"ref",
"street",
"telephone",
"terrace",
"welcomes",
"____",
"ater",
"com",
"con",
"en",
"fig",
"fo",
"hello",
"ion",
"les",
"ll",
"lo",
"ns",
"redacted",
"tions",
"tt",
"ve",
"167",
"1944",
"associate",
"association",
"box",
"bradbury",
"cr",
"emma",
"inc",
"officer",
"po",
"shire",
"www.mda.asn.au",
"10.1080",
"141",
"145",
"146",
"159",
"163",
"265",
"315",
"342",
"349",
"359",
"664",
"722",
"academy",
"institute",
"phd",
"professor",
"r.j",
"r.m",
"sciences",
"society",
"university",
"acn",
"10.1111",
"al",
"doi",
"e.g",
"eds",
"et",
"pp",
"8003"
)

extended_stopwords <- c(
  genre_words,
  pdf_processing_words,
  stopwords("english")
)

submission_tokens <- tokens(
    submission_corpus, remove_punct=TRUE, split_hyphens = TRUE, remove_symbols = TRUE
  ) %>% 
  tokens_remove(extended_stopwords, min_nchar=2)

# Only consider words that appear in at least 10 submissions overall.
submissions_dfm <- dfm_trim(dfm(submission_tokens), min_docfreq=10)

# This reduces every wordcount > 1 to 1.
submissions_boolean <- dfm_weight(submissions_dfm, scheme="boolean")

score_keywords <- function(keyword_dfm, corpus_variable_name, variable) {
  scores <- textstat_keyness(keyword_dfm, docvars(submission_corpus, corpus_variable_name) == variable, "chi") %>%
    mutate(rank=rank(desc(chi2), ties.method="random"))
  scores$variable_name <- corpus_variable_name
  scores$variable <- variable
  return(scores)
}

# Accumulate scores for each stakeholder group.
submitter_labels <- unique(submissions$label)

results <- list()
for (label in submitter_labels) {
    scores = score_keywords(submissions_boolean, 'label', label)
    scores$keyword_from <- label
    results <- append(results, list(scores))
  }

# Filter and merge all of the lists to identify the top k keywords for each slice
all_scores = bind_rows(results)
keyword_sources = all_scores %>%
  filter(rank <= 50) %>% 
  select(feature, keyword_from, rank)

# Pivot to wide form with one row per feature to allow cross cutting comparison
keywords_final <- pivot_wider(
  keyword_sources, 
  names_from=c("keyword_from"), 
  values_from=c("feature"),
  id_cols = c("rank")
)

# Version for detailed examination
write_xlsx(keywords_final, "../results/initial_keywords.xlsx")

print(xtable(keywords_final %>% select(!rank)), include.rownames=FALSE)
```

# Keyword collocates

To contextualise each keyword we will extract the collocating words. A word is
a colocate with a keyword if it is statistically more likely to occur within
10 words of the keyword than in other places in the submissions. Note that this
block generates all of the collocations in bulk - the following section will
combine collocations with rel freq across groups to allow some limited comparative
analysis.

```{r}

# Extract collocations of each keyword in that specific group.
extract_collocations <- function(i) {
  feature <- keyword_sources$feature[i]
  group <- keyword_sources$keyword_from[i]
  
  # Look at tokens only for the documents in the group: otherwise we mix keyword
  # meanings from other groups.
  group_tokens = submission_tokens[docvars(submission_corpus, 'label') == group]
  
  # Construct two objects: one containing just the regions
  # around the keyword of interest, the other all of the
  # other regions (both removing the keywords themselves).
  # Then the collocation check is a keyword test.
  # See: https://tutorials.quanteda.io/advanced-operations/target-word-collocations/
  windowsize <- 10
  tokens_inside <- tokens_keep(
    group_tokens, 
    pattern=feature, 
    valuetype='fixed', 
    # Note that window is after removing stop words and punctuation, so
    # is effectively longer than you might first think.
    window=windowsize
  )
  tokens_inside <- tokens_remove(
    tokens_inside, pattern=feature, valuetype='fixed'
  )
  tokens_outside <- tokens_remove(
    group_tokens, pattern=feature, valuetype='fixed', window=windowsize
  )
  
  # We're determining collocations only for the group that generated
  # the keyword so we can understand it in context for that group.
  # Note that we'll trim any collocation that doesn't occur with the target
  # word at least twice, to not score rare co-locations.
  dfm_inside <- dfm(tokens_inside)
  dfm_outside <- dfm(tokens_outside)
                         
  group_score <- textstat_keyness(
    rbind(dfm_inside, dfm_outside), 
    target = seq_len(ndoc(dfm_inside))
  ) %>% 
  filter(n_target > 2) %>% # Make sure to filter out if there are few colocates.
  slice(1:10) %>% 
  mutate(rank=rank(desc(chi2)))
  
  # Handle case of zero collocations - otherwise we get an error trying to
  # assign a single value for the feature to an otherwise empty dataframe. If
  # there's more than zero collocates the single value is broadcast to the
  # full size of the df.
  if (nrow(group_score) > 0) {
    group_score$keyword = feature
    group_score$keyword_from = group
  } else {
    group_score$keyword = character()
    group_score$keyword_from = character()
  }
  
  return(group_score)
}

collocations <- bind_rows(lapply(seq_along(keyword_sources$feature), extract_collocations))

wide_grouped_ranked_collocates <- collocations %>%
  group_by(keyword_from, keyword) %>%
  arrange(rank, .by_group = TRUE) %>%
  summarise(collocated_features = paste0(feature, collapse=", "))

# Also extract a sample of concordance lines for each keyword in its 
# own group and all other groups.

write_xlsx(wide_grouped_ranked_collocates, "../results/keywords_with_collocates.xlsx")

```

# Keyword Relative Frequency Across Groups and collocation summary

In order to compare the frequency of keywords across groups it is useful to
look at just the selected set of keywords.

``` {r}
# Count number of submissions from each group containing the words
group_word_counts <- dfm_group(submissions_boolean, groups = label) %>% 
  convert('data.frame') %>%
  rename(label=doc_id) %>%
  pivot_longer(!label, values_to="count", names_to="feature") %>%
  select(label, feature, count)

# Total document counts for each group 
group_docs <- submissions %>% count(label)

# Relative frequencies within each group of submitters
relative_frequencies <- group_word_counts %>% 
  inner_join(group_docs, by = c("label")) %>%
  mutate(relative_freq = 100 * count / n)

# Note that we're only ever going to plot a specific subset of these words
# at a time - otherwise this will be completely overwhelming.
plot_groupwise_rel_freqs <- function (args) {
  highlight_group = args[[1]]
  keywords = args[[2]]
  subgroup_no = args[[3]]
  figure_order = args[[4]]
  
  # Make sure the output directories exist
  output_group = paste(c("../results/by_group/", highlight_group), collapse="")
  if (!dir.exists(output_group)) {dir.create(output_group)}
    
  output_subgroup = paste(c(output_group, "/", subgroup_no), collapse="")
  if (!dir.exists(output_subgroup)) {dir.create(output_subgroup)}
  
  
  subset <- relative_frequencies %>% filter(feature %in% keywords, label!='notcategorisable')
  subset$selected_group <- subset$label == highlight_group

  subset$feature <- factor(subset$feature, levels=arrange(filter(subset, label==highlight_group), desc(relative_freq))$feature)
  
  small_multi <-
    ggplot(subset, aes(y=relative_freq, x=label, fill=selected_group)) +
    guides(fill="none") +
    geom_col() +
    facet_grid(rows=vars(feature), switch="y") +
    theme_minimal() +
    theme(axis.title.x=element_blank(), 
          panel.spacing.x=unit(0.2, "cm"), 
          panel.spacing.y=unit(0.2, "cm"),
          strip.text.y.left = element_text(angle = 0)) +
    scale_y_continuous(name="%", position="right") +
    scale_x_discrete(name="Stakeholder Group", guide = guide_axis(angle = 45)) +
    geom_hline(yintercept = 0, color = "#666666", linewidth = 0.5) 
  
    filename <- paste(c(output_subgroup, "/", figure_order, "_", keywords[1], ".pdf"), collapse="")
    print(filename)
    ggsave(filename, plot = small_multi, width=90/25.4, height=(length(keywords)*8/25.4) + 40/25.4)
  return(small_multi)
}

create_latex_table <- function(args) {
  highlight_group = args[[1]]
  keywords = args[[2]]
  subgroup_no = args[[3]]
  figure_order = args[[4]]
  
  # This runs after the figure creation so the folders will exist already.
  output_subgroup = paste(c("../results/by_group/", highlight_group, "/", subgroup_no), collapse="")
  filename <- paste(c(output_subgroup, "/", figure_order, "_", keywords[1], ".html"), collapse="")

  selected_words <- wide_grouped_ranked_collocates %>% 
    filter(keyword_from==highlight_group, keyword %in% keywords) %>%
    inner_join(relative_frequencies, 
               by=join_by(keyword_from==label, keyword==feature)) %>%
    arrange(desc(relative_freq)) %>% ungroup %>%
    select(keyword, collocated_features)

  print(xtable(selected_words), file="../results/latex_tables.txt", include.rownames=FALSE, append=TRUE)
  print(xtable(selected_words), type = "html", file = filename)
  return(selected_words)
}

groups_to_visualise <- list(
  # commercial, non-consumptive
  list('commercialnon', c("clients", "intermediaries", "participants"), "1", "a"),
  list('commercialnon', c("opaque", "scarcity"), "1", "b"),
  list('commercialnon', c("bids",
    "buyer",
    "buyers",
    "contract",
    "incentivised",
    "indemnity",
    "investors",
    "ivt",
    "liquidity"), "2", "a"),
  
  # consumptive water users
  list('consumptive', c("family", "farmer", "farming"), "1", "a"),
  list('consumptive', c("dairy", "cattle", "livestock"), "1", "b"),
  
  list('consumptive', c("450gl", "650gl", "ml"), "2", "a"),
  list('consumptive', c("permanent", "temporary", "entitlement", "carryover"), "2", "b"),
  list('consumptive', c("speculators", "traders"), "2", "c"),
  
  list('consumptive', c("barley", "cereals", "planted"), "3", "a"),
  list('consumptive', c("feral"), "3", "b"),
  
  # elected reps
  list('electedrep', c("1,500", "2,750", "450", "gigalitres"), "1", "a"),  
  list('electedrep', c("overallocation"), "1", "b"),
  
  # resource managers
  list('resourcemanagers', c("utilities", "wastewater", "stormwater", "potable"), "1", "a"),
  list('resourcemanagers', c("customer", "customers"), "1", "b"),
  list('resourcemanagers', c("covid", "pandemic"), "1", "c"),

  # regional
  list('regional', c("resident", "lived", "lga", "km2"), "1", "a"),
  list('regional', c("councils", "interagency", "referenced", "website"), "1", "b"),
  
  list('regional', c("390gl", "320gl"), "2", "a"),
  list('regional', c("tourism"), "2", "b"),
  list('regional', c("equitable", "greedy", "wellbeing"), "2", "c"),
  
  # First Nations
  list('firstnations', c("aboriginal", "indigenous", "lands", "peoples"), "1", "a"),
  list('firstnations', c("culture", "spiritual", "traditions", "sacred", "customs"), "1", "b"),
  list('firstnations', c("foods", "hunting", "medicine"), "1", "c"),
  
  list('firstnations', c("nban", "mldrin"), "2", "a"),
  list('firstnations', c("colonisation", "dispossessed", "neoliberal", "nullius"), "2", "b"),
  list('firstnations', c("kon", "undrip", "title"), "2", "c"),
  
  
  # Government
  list('government', c("accordance",
    "assurance",
    "benchmark",
    "coordinated",
    "coordination",
    "undergone",
    "undertaken"),
    "1",
    "a"
  ),
  
  # Research
  list('research', c("journal", "australasian"), "1", "a"),
  list('research', c("decentralised", "scales", "linear"), "1", "b"),
  list('research', c("sensing", "landsat"), "2", "a"),
  list('research', c("mortality", "stressors", "biota"), "2", "b"),
  
  # Environmental
  list('environmental', c(
    "415",
    "available",
    "gwydir",
    "reduction",
    "significant",
    "species",
    "stress"
    ),
    "2", 
    "a"
  ),
  list('environmental', c("migratory", "ramsar"), "1", "c"),
  list('environmental', c(
    "birds",
    "fish",
    "wildlife"
    ),
    "1",
    "a"
  ),
  list('environmental', c(
    "ecological",
    "vulnerable",
    "stress",
    "wetlands",
    "marshes"),
    "1",
    "b"
  )
)

output_dir = "../results/by_group"
if (!dir.exists(output_dir)) {dir.create(output_dir)}

# Clear out the table file of previous results
write("", file="../results/latex_tables.txt")
lapply(groups_to_visualise, plot_groupwise_rel_freqs)
lapply(groups_to_visualise, create_latex_table)

```

# Concordances

In this final part we will construct matching concordances within each group for
the keywords. These complement the collocates by indicating how keywords are 
used in each group, and allows us to do cross comparison of words in different
groups to understand whether keywords are used the same or differently.

```{r}
# Don't remove stopwords and punctuation for this phase as we want to generate
# readable concordance lines.
raw_tokens = tokens(submission_corpus,  split_hyphens = TRUE)

# Create a sample of concordances for selected words in each group.
single_word_group_concordance <- function(i, groups) {
  
  feature = keyword_sources$feature[i]
  keyword_from = keyword_sources$keyword_from[i]
  
  results <- list()
  
  for (group in groups) {
    
    if (group != 'environmental') {
      tokens_subset = raw_tokens[docvars(submission_corpus, 'label') == group]
      # Find matches, but don't generate full concordances yet
      idx = index(tokens_subset, feature, valuetype="fixed")
      # Subsample matches
      idx <- idx[sample(nrow(idx), min(nrow(idx), 20)),]
      concordances <- kwic(
        tokens_subset, index=idx, window=20
      ) %>% as.data.frame()
    } else {
      # Special case the environmental submissions to handle the multiple 
      # duplicates by also taking concordances from elsewhere. We'll take
      # a sample of 10 concordance lines from the mdba_bpa_2017, 10 from
      # all other inquiries.
      tokens_mdba = raw_tokens[docvars(submission_corpus, 'label') == group & docvars(submission_corpus, 'inquiry_shortname') == 'mdba_bpa_2017']
      tokens_other = raw_tokens[docvars(submission_corpus, 'label') == group & docvars(submission_corpus, 'inquiry_shortname') != 'mdba_bpa_2017']
      
      idx_mdba = index(tokens_mdba, feature, valuetype="fixed")
      idx_other = index(tokens_other, feature, valuetype="fixed")
      # Subsample matches
      idx_mdba <- idx_mdba[sample(nrow(idx_mdba), min(nrow(idx_mdba), 10)),]
      idx_other <- idx_other[sample(nrow(idx_other), min(nrow(idx_other), 10)),]
      
      concordances_mdba <- kwic(
        tokens_mdba, index=idx_mdba, window=20
      ) %>% as.data.frame()
      
      concordances_other <- kwic(
        tokens_other, index=idx_other, window=20
      ) %>% as.data.frame()
      
      concordances <- concordances_mdba %>% union(concordances_other)
      
    }

    if (nrow(concordances) > 0) {
      concordances$merged <- paste(concordances$pre, concordances$keyword, concordances$post)
      concordances$keyword_from <- keyword_from
      concordances$concordances_for <- group
      concordances <- concordances %>% 
        select(docname, keyword_from, concordances_for, pattern, pre, keyword, post, merged)
      
      results <- append(results, list(concordances))
    }
  }
  
  return(bind_rows(results))
}

concordances <- bind_rows(lapply(seq_along(keyword_sources$feature), single_word_group_concordance, groups=unique(keyword_sources$keyword_from)))

write_xlsx(concordances, "../results/keyword_group_concordances.xlsx")

```

# Collocations across groups for selected words

For a selected group of words that are used in different ways across groups
it's useful to capture the variation in collocations as an indicator of 
variation in meanings.

```{r}

# Generate the collocations of the selected words, both across the entire
# group of submissions and specific groups.
generate_group_collocates <- function(features) {
  ranked_collocates <- list()
  
  windowsize <- 10
  top_n <- 20
  
  # First compute the collocations over all groups.
  for (feature in features) {  
    # Construct two objects: one containing just the regions
    # around the keyword of interest, the other all of the
    # other regions (both removing the keywords themselves).
    # Then the collocation check is a keyword test.
    # See: https://tutorials.quanteda.io/advanced-operations/target-word-collocations/
    tokens_inside <- tokens_keep(
      submission_tokens, 
      pattern=feature, 
      valuetype='fixed', 
      # Note that window is after removing stop words and punctuation, so
      # is effectively longer than you might first think.
      window=windowsize
    )
    tokens_inside <- tokens_remove(
      tokens_inside, pattern=feature, valuetype='fixed'
    )
    tokens_outside <- tokens_remove(
      submission_tokens, pattern=feature, valuetype='fixed', window=windowsize
    )
    
    # We're determining collocations only for the group that generated
    # the keyword so we can understand it in context for that group.
    # Note that we'll trim any collocation that doesn't occur with the target
    # word at least twice, to not score rare co-locations.
    dfm_inside <- dfm(tokens_inside)
    dfm_outside <- dfm(tokens_outside)
                           
    group_score <- textstat_keyness(
      rbind(dfm_inside, dfm_outside), 
      target = seq_len(ndoc(dfm_inside))
    ) %>% 
    filter(n_target > 2) %>% # Make sure to filter out if there are few collocates.
    slice(1:top_n) %>% 
    mutate(rank=rank(desc(chi2)))
    
    # Handle case of zero collocations - otherwise we get an error trying to
    # assign a single value for the feature to an otherwise empty dataframe. If
    # there's more than zero collocates the single value is broadcast to the
    # full size of the df.
    if (nrow(group_score) > 0) {
      group_score$keyword = feature[[1]]
      group_score$keyword_from = 'all'
      ranked_collocates <- append(ranked_collocates, list(group_score))
    } 
  }
  
  for (group in unique(submissions$label)) {
    
    # Look at tokens only for the documents in the group: otherwise we mix keyword
    # meanings from other groups.
    group_tokens = submission_tokens[docvars(submission_corpus, 'label') == group]
    
    for (feature in features) {  
      # Construct two objects: one containing just the regions
      # around the keyword of interest, the other all of the
      # other regions (both removing the keywords themselves).
      # Then the collocation check is a keyword test.
      # See: https://tutorials.quanteda.io/advanced-operations/target-word-collocations/
      tokens_inside <- tokens_keep(
        group_tokens, 
        pattern=feature, 
        valuetype='fixed', 
        # Note that window is after removing stop words and punctuation, so
        # is effectively longer than you might first think.
        window=windowsize
      )
      tokens_inside <- tokens_remove(
        tokens_inside, pattern=feature, valuetype='fixed'
      )
      tokens_outside <- tokens_remove(
        group_tokens, pattern=feature, valuetype='fixed', window=windowsize
      )
      
      # We're determining collocations only for the group that generated
      # the keyword so we can understand it in context for that group.
      # Note that we'll trim any collocation that doesn't occur with the target
      # word at least twice, to not score rare co-locations.
      dfm_inside <- dfm(tokens_inside)
      dfm_outside <- dfm(tokens_outside)
                             
      group_score <- textstat_keyness(
        rbind(dfm_inside, dfm_outside), 
        target = seq_len(ndoc(dfm_inside))
      ) %>% 
      filter(n_target > 2) %>% # Make sure to filter out if there are few collocates.
      slice(1:top_n) %>% 
      mutate(rank=rank(desc(chi2)))
      
      # Handle case of zero collocations - otherwise we get an error trying to
      # assign a single value for the feature to an otherwise empty dataframe. If
      # there's more than zero collocates the single value is broadcast to the
      # full size of the df.
      if (nrow(group_score) > 0) {
        group_score$keyword = feature[[1]]
        group_score$keyword_from = group
        ranked_collocates <- append(ranked_collocates, list(group_score))
      } 
    }
  }
  return (bind_rows(ranked_collocates))
}

# Extract collocations of each selected feature across all groups
chosen_features = list(
  "water", 
  "community", 
  "communities", 
  "crisis",
  c(
    "fairness", 
    "unfair",
    "unfairness",
    "unfairly",
    "fair",
    "equity", 
    "equitable",
    "inequitable",
    "inequity",
    "unequitable",
    "justice",
    "injustice",
    "injustices",
    "unjust"
  )
)
selected_collocates <- generate_group_collocates(chosen_features)

wide_selected_ranked_collocates <- selected_collocates %>%
  group_by(keyword_from, keyword) %>%
  arrange(rank, .by_group = TRUE) %>%
  summarise(collocated_features = paste0(feature, collapse=", "))


write_xlsx(selected_collocates, "../results/selected_allgroup_collocates.xlsx")
write_xlsx(wide_selected_ranked_collocates, "../results/wide_selected_allgroup_collocates.xlsx")

```

# Concordances across groups for separate words

``` {r}

# Don't remove stopwords and punctuation for this phase as we want to generate
# readable concordance lines.
raw_tokens = tokens(submission_corpus,  split_hyphens = TRUE)

# Create a sample of concordances for selected words in each group.
# This driven solely by what has been selected, not the existing keywords.
single_chosen_word_group_concordance <- function(feature, groups) {
  
  results <- list()
  
  for (group in groups) {
    
    if (group != 'environmental') {
      tokens_subset = raw_tokens[docvars(submission_corpus, 'label') == group]
      # Find matches, but don't generate full concordances yet
      idx = index(tokens_subset, feature, valuetype="fixed")
      # Subsample matches
      idx <- idx[sample(nrow(idx), min(nrow(idx), 20)),]
      concordances <- kwic(
        tokens_subset, index=idx, window=20
      ) %>% as.data.frame()
    } else {
      # Special case the environmental submissions to handle the multiple 
      # duplicates by also taking concordances from elsewhere. We'll take
      # a sample of 10 concordance lines from the mdba_bpa_2017, 10 from
      # all other inquiries.
      tokens_mdba = raw_tokens[docvars(submission_corpus, 'label') == group & docvars(submission_corpus, 'inquiry_shortname') == 'mdba_bpa_2017']
      tokens_other = raw_tokens[docvars(submission_corpus, 'label') == group & docvars(submission_corpus, 'inquiry_shortname') != 'mdba_bpa_2017']
      
      idx_mdba = index(tokens_mdba, feature, valuetype="fixed")
      idx_other = index(tokens_other, feature, valuetype="fixed")
      # Subsample matches
      idx_mdba <- idx_mdba[sample(nrow(idx_mdba), min(nrow(idx_mdba), 10)),]
      idx_other <- idx_other[sample(nrow(idx_other), min(nrow(idx_other), 10)),]
      
      concordances_mdba <- kwic(
        tokens_mdba, index=idx_mdba, window=20
      ) %>% as.data.frame()
      
      concordances_other <- kwic(
        tokens_other, index=idx_other, window=20
      ) %>% as.data.frame()
      
      concordances <- concordances_mdba %>% union(concordances_other)
      
    }

    if (nrow(concordances) > 0) {
      concordances$merged <- paste(concordances$pre, concordances$keyword, concordances$post)
      concordances$concordances_for <- group
      concordances <- concordances %>% 
        select(docname, concordances_for, pattern, pre, keyword, post, merged)
      
      results <- append(results, list(concordances))
    }
  }
  
  return(bind_rows(results))
}

concordances <- bind_rows(
  lapply(
    unlist(chosen_features, recursive = FALSE), 
    single_chosen_word_group_concordance, 
    groups=unique(keyword_sources$keyword_from)
    )
  )

write_xlsx(concordances, "../results/selected_concordances_by_group.xlsx")


```
