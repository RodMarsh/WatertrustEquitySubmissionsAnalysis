# Keyword Density Plots

These graphs show how keyword usage varies by submission group. Each plot is
like a `fingerprint` of the specific language they use and how that compared
to other groups of submitters.

```{r eval=FALSE}
install.packages("quanteda")
install.packages("quanteda.textstats")
install.packages("RSQLite")
install.packages("writexl")
```

# Load and prepare are submission data as extracted from the various inquiries
```{r}
library(DBI)
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
library(writexl)
library(readxl)

inquiries <- dbConnect(RSQLite::SQLite(), "../inquiries/inquiries.db")
submissions <- dbGetQuery(inquiries, 
  'SELECT submission.*, submission_label.label FROM submission
  inner join submission_label using(inquiry_shortname, submission_id)
  where priority = 1
  '
) %>% mutate(text = gsub("[‘’]", "'", text))

# Generate a single column unique id - this is the composite primary key
# defined in the database, so will always be unique. This is used to tie
# together results at different granularities.
submissions$doc_id <- paste(submissions$inquiry_shortname, submissions$submission_id)
submission_corpus <- corpus(submissions, text_field="text", docid_field = "doc_id")

dbDisconnect(inquiries)

print(submission_corpus)
```

# Keyword Density Analysis Preparation

This analysis looks at the fraction of documents from each submission group that
use particular keywords: the keywords for each group then act as a signature
for the submissions from that group, and the similarity or not with other
groups shows how much particular words are or aren't present.

This block counts the relative number of submissions in each group that use
each keyword.

```{r}
# The input here is a two column table: the keyword column has the keyword,
# the label column has the submitter group for that keyword. Note that a word
# can be used for multiple groups.
word_groups <- read_xlsx("../results/keyword_scores.xlsx")

# Use the same stopword list as for the keyword detection.
extended_stopwords <- c(
  'pty', 
  'ltd',
  'pp',
  'doi',
  'e.g',
  'et',
  'al',
  'eds',
  'press',
  'journal',
  '10.1111',
  'image',
  # 'height',
  'width',
  'rgb',
  'srgb',
  'bpc',
  '16',
  'iccbased',
  # 'water',
  'eds',
  'mr',
  '2017',
  '2013',
  '41',
  '95',
  '87',
  '8003',
  '2006',
  '2018',
  '2601',
  '02',
  '43',
  stopwords("english")
)

submission_tokens <- tokens(
    submission_corpus, remove_punct=TRUE, split_hyphens = TRUE, remove_symbols = TRUE
  ) %>% 
  tokens_remove(extended_stopwords, min_nchar=2)

# Apply keyword selection early as we don't need to work with any of the other cols
submissions_dfm <- dfm_select(dfm(submission_tokens), pattern = word_groups$feature)
# This reduces every wordcount > 1 to 1.
submissions_boolean <- dfm_weight(submissions_dfm, scheme="boolean")

# This is the submission counts of each word aggregated by group label
# We pivot from the dfm to long format with one row per keyword/group
group_word_counts <- dfm_group(submissions_boolean, groups = label) %>% 
  convert('data.frame') %>%
  rename(label=doc_id) %>%
  pivot_longer(!label, values_to="count", names_to="feature") %>%
  inner_join(word_groups, by="feature") %>%
  select(label, feature, count, keyword_from)

# Total document counts for each group 
group_docs <- submissions %>% count(label)
group_order <- group_docs %>% arrange(n)

# Relative frequencies within each group of submitters
relative_frequencies <- group_word_counts %>% 
  inner_join(group_docs, by = c("label")) %>%
  mutate(relative_freq = 100 * count / n)

# Drop words that don't reach a relative frequency threshold in at least one 
# group. This means that a keyword needs to occur in at least 10% of submissions
# from at least one group to be considered important enough to pass through.
above_rel_freq_thres <- relative_frequencies %>%
  group_by(feature) %>% 
  slice_max(relative_freq, with_ties=FALSE) %>%
  filter(relative_freq >= 20) %>%
  select(feature)

filtered_rel_freq <- relative_frequencies %>% 
  inner_join(above_rel_freq_thres, by=c('feature'))

```

# Visualisation: small multiple barcharts for each group.

```{r}
label_order <- c(
  'consumptive',
  'environmental',
  'firstnations',
  'government',
  'resourcemanagers',
  'regional',
  'commercialnon',
  'research',
  'notcategorisable'
)
# Adjust group order for the graph
filtered_rel_freq$label <- factor(filtered_rel_freq$label, levels=label_order)

make_small_multiples <- function (keyword_group) {
  subset <- filtered_rel_freq %>% filter(keyword_from == keyword_group)
  # Reorder feature in desc relative freq for this label group (if the keyword group is a label).
  if (keyword_group %in% relative_frequencies$label) {
    subset$feature <- factor(subset$feature, levels=arrange(filter(subset, label==keyword_group), relative_freq)$feature)
    subset$selected_group <- subset$label == keyword_group
    small_multi <- 
      ggplot(subset, aes(y=relative_freq, x=feature, fill=selected_group)) +
      guides(fill="none") + 
      geom_col() +
      coord_flip() +
      facet_grid(. ~ label) +
      theme_minimal() +
      theme(panel.spacing.x=unit(0, "cm"), panel.spacing.y=unit(0, "cm")) +
      scale_y_continuous(name="Percentage of Group Submissions", limits=c(0, 100), breaks=c(0, 25, 50, 75)) +
      geom_hline(yintercept = 0, color = "#666666", linewidth = 0.5)
    ggsave(paste("../results/keyword_smallmultiples_", keyword_group,".pdf"), plot = small_multi, width = 297/25.4, height =  210/25.4)
  return(small_multi)
  }
}

lapply(unique(relative_frequencies$keyword_from), make_small_multiples)


```

# Selected keyword summary

```{r}
# Note we're setting these up as factors so as to be in the correct order for 
# display
labels <- c(
  'consumptive',
  'environmental',
  'firstnations',
  'government',
  'resourcemanagers',
  'research',
  'regional',
  # Note that we're actually not choosing these for anything, we just need
  # the factors to have the same sets
  'commercialnon',
  'notcategorisable'
)


# Note - keywords are repeated in this group to match multiple submitter groups
keywords <- c(
  'farm',
  'family',
  'food',
  'irrigation',
  'plan',
  'wetlands',
  'ecosystems',
  'fish',
  'flows',
  'protect',
  'cultural',
  'vulnerable',
  'land',
  'country', 
  'rights',
  'us',
  'cultural',
  'care',
  'plan',
  'outcomes',
  'measures',
  'framework',
  'efficiency',
  'pricing',
  'planning',
  'regulation',
  'customers',
  'efficiently',
  'framework',
  'research',
  'flow',
  'plan',
  'ecological',
  'university',
  'professor',
  'crisis',
  'plan',
  'river',
  'communities',
  'government',
  'local',
  'region'
)

keyword_labels <- c (
  'consumptive',
  'consumptive',
  'consumptive',
  'consumptive',
  'consumptive',
  'environmental',
  'environmental',
  'environmental',
  'environmental',
  'environmental',
  'environmental',
  'environmental',
  'firstnations',
  'firstnations',
  'firstnations',
  'firstnations',
  'firstnations',
  'firstnations',
  'government',
  'government',
  'government',
  'government',
  'government',
  'resourcemanagers',
  'resourcemanagers',
  'resourcemanagers',
  'resourcemanagers',
  'resourcemanagers',
  'resourcemanagers',
  'research',
  'research',
  'research',
  'research',
  'research',
  'research',
  'research',
  'regional',
  'regional',
  'regional',
  'regional',
  'regional',
  'regional'
)

include_keywords <- data.frame(feature=keywords, selected_for=factor(keyword_labels, labels))

selected_subset <- filtered_rel_freq %>%
  filter(feature %in% include_keywords$feature, label %in% keyword_labels) %>%
  select(feature, label, relative_freq) %>%
  distinct() %>%
  inner_join(include_keywords, by='feature', relationship='many-to-many') %>%
  mutate(keyword_fill = "bg", highlight=ifelse(selected_for==label, 'highlight', 'bgalpha'))


# Order each keyword by it's max relative frequency within one group.
keyword_order <- selected_subset %>% 
  filter(selected_for == label) %>%
  group_by(feature) %>%
  slice_max(tibble(relative_freq, label), with_ties=FALSE) %>%
  arrange(as.integer(selected_for), desc(relative_freq))

# Note we reverse the keyword order in the factor, because we've already coord_flipped the axes,
# otherwise we'd have the first keyword down the bottom.
selected_subset$feature <- factor(selected_subset$feature, rev(keyword_order$feature))
selected_subset$label <- factor(selected_subset$label, labels)

highlight_colors <- c("bg"="#F8766D", "highlight"='#000000', 'bgalpha'='#11111100')

small_multi <- ggplot(selected_subset, aes(y=relative_freq, x=feature, fill=keyword_fill, color=highlight)) +
  guides(fill="none", color="none") + 
  geom_col() +
  coord_flip() +
  facet_grid(. ~ label) +
  theme_minimal() +
  theme(panel.spacing.x=unit(0, "cm"), panel.spacing.y=unit(0, "cm")) +
  scale_y_continuous(name="Percentage of Submissions", limits=c(0, 100), breaks=c(0, 25, 50, 75)) +
  geom_hline(yintercept = 0, color = "#666666", linewidth = 0.5) +
  scale_fill_manual(values=highlight_colors) +
  scale_colour_manual(values=highlight_colors)

small_multi

ggsave(paste("../results/keyword_smallmultiples_summary.pdf"), plot = small_multi, width = 210/25.4, height =  0.5*297/25.4)

```


# Drilldown visualisation: word cooccurrence networks

The aim of this visualisation is to extend the keyword tabulations above and allow
drilling down into the context of the chosen keywords: we aim to see the words
that co-occur with the keywords broken down by groups.

```{r}

illustrative_words <- tibble(
  feature = c(
    "water",
    "rivers",
    "flows",
    "communities"
  ),
  keyword_from = "selected"
)

group_keywords <- filtered_rel_freq %>% 
  filter(label == keyword_from) %>%
  select(keyword_from, feature) %>%
  union(illustrative_words)


# For each of the keywords remaining after filtering, find the co-occurring words
# in each group of submissions. Each of these patterns is then a descriptive
# vector of interesting things to consider for what that word means to that
# group.
keyword_group_ranks <- list()
for (word in unique(group_keywords$feature)) {
  # Make the binary weighted token windows for the selected words
  token_windows <- tokens_select(
    submission_tokens, 
    pattern=word, 
    valuetype='fixed', 
    # Note that window = 10 is after removing stop words and punctuation, so
    # is effectively longer than you might first think.
    window=10
  )
  doc_dfm <- dfm_weight(dfm(token_windows), scheme='boolean')
  for (group in chosen_groups) {
    # Score that DFM for each group for the chosen word.
    group_score <- textstat_keyness(
      doc_dfm, target=docvars(submission_corpus, 'label') == group
    ) %>% slice(1:20)
    
    group_score$keyword <- word
    group_score$group <- group
    keyword_group_ranks <- append(keyword_group_ranks, list(group_score))
  }
}

ranked_collocates <- bind_rows(keyword_group_ranks)

grouped_ranked_collocates <- ranked_collocates %>% 
  inner_join(group_keywords, by=join_by(keyword==feature))

# Form the final results and glue back the original source of the keyword...
# Note that some words are keywords from multiple groups and are therefore
# *repeated* in the data.



write_xlsx(grouped_ranked_collocates, "../results/keyword_group_collocation_ranks.xlsx")


```

