# Topic modelling of submissions to identify frames

Can we triangulate the frames identified by the actor-keyword analysis through
the application of topic modelling across all submissions? The aim of this is
to explore how frames may persist or be used across different actor groups, 
rather than be specifically tied into or constructed by specific groups.

```{r eval=FALSE}
install.packages("RSQLite")
install.packages("writexl")
install.packages("tidyverse")
install.packages("seededlda")
install.packages("xtable")
```

# Load and prepare are submission data as extracted from the various inquiries
```{r}
library(DBI)
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
library(seededlda)
library(writexl)
library(xtable)

inquiries <- dbConnect(RSQLite::SQLite(), "../inquiries/inquiries.db")
submissions <- dbGetQuery(inquiries, 
  'SELECT submission.*, submission_label.label, context FROM submission
  inner join submission_label using(inquiry_shortname, submission_id)
  inner join inquiry using(inquiry_shortname)
  where priority = 1
  '
) %>% mutate(text = gsub("[‘’]", "'", text))

# Generate a single column unique id - this is the composite primary key
# defined in the database, so will always be unique. This is used to tie
# together results at different granularities.
submissions$doc_id <- paste(submissions$inquiry_shortname, submissions$submission_id)
submission_corpus <- corpus(submissions, text_field="text", docid_field = "doc_id")

dbDisconnect(inquiries)

```

# Construct document-feature matrix then topic models

The topic models here are not apriori constructed: this is done after our 
detailed analysis of keywords and actor-groups. The way we interpret these
models is therefore informed by this previous close reading and analysis. 

Note that the feature processing and tokenisation pipeline is the same as used
in the concordance and keyword analysis.

```{r}

# These are the byproducts of extracting text from the PDF where images are
# included.
pdf_processing_words <- c(
  'image',
  'height',
  'width',
  'rgb',
  'srgb',
  'bpc',
  '16',
  'iccbased'
)
# These words are potential keywords, but reflect the genre and form of the
# submission, not the substantive content.
genre_words <- c(
  'pty', 
  'ltd',
"etc",
"re",
"amendment",
"bill",
"chamber",
"coalition",
"commend",
"debate",
"deputy",
"election",
"electorate",
"greens",
"hanson",
"labor",
"legislation",
"liberal",
"minister",
"opposition",
"prime",
"rise",
"said",
"say",
"saying",
"senator",
"something",
"speak",
"speaker",
"speech",
"sure",
"talk",
"talked",
"talking",
"things",
"think",
"today",
"want",
"went",
"authority",
"authority",
"basin",
"darling",
"sincerely",
"submission",
"067",
"1801",
"bag",
"compiling",
"contact",
"cr",
"fax",
"gpo",
"hon",
"manager",
"mayor",
"mlc",
"mp",
"nicholson",
"ref",
"street",
"telephone",
"terrace",
"welcomes",
"____",
"ater",
"com",
"con",
"en",
"fig",
"fo",
"hello",
"ion",
"les",
"ll",
"lo",
"ns",
"redacted",
"tions",
"tt",
"ve",
"167",
"1944",
"associate",
"association",
"box",
"bradbury",
"cr",
"emma",
"inc",
"officer",
"po",
"shire",
"www.mda.asn.au",
"10.1080",
"141",
"145",
"146",
"159",
"163",
"265",
"315",
"342",
"349",
"359",
"664",
"722",
"academy",
"institute",
"phd",
"professor",
"r.j",
"r.m",
"sciences",
"society",
"university",
"acn",
"10.1111",
"al",
"doi",
"e.g",
"eds",
"et",
"pp",
"8003"
)

extended_stopwords <- c(
  genre_words,
  pdf_processing_words,
  stopwords("english")
)

submission_tokens <- tokens(
    submission_corpus, remove_punct=TRUE, split_hyphens = TRUE, remove_symbols = TRUE
  ) %>% 
  tokens_remove(extended_stopwords, min_nchar=2)

# Only consider words that appear in at least 10 submissions overall.
submissions_dfm <- dfm_trim(dfm(submission_tokens), min_docfreq=10)

# Generate the topic model, with a large number of topics to enable filtering.
submissions_lda <- textmodel_lda(submissions_dfm, k=50)
results <- terms(submissions_lda, n=40) %>% 
  as.data.frame() %>%
  pivot_longer(everything(), names_to="topic") %>%
  group_by(topic) %>%
  summarise(features = paste0(value, collapse=", "))

# Organise topics by prevalence - this is the sum of all of the topic proportions
# for each topic - higher numbers indicate that proportionally more of documents
# were generated by drawing from that topic mixture.
relative_prevalence <- colSums(submissions_lda$theta) / sum(submissions_lda$theta) %>%
  sort(decreasing=TRUE) 

# convert named vector back to a long format data frame
relative_prevalence <- cbind(read.table(text = names(relative_prevalence)), relative_prevalence)

# Glue back together
topic_word_prevalence <- results %>% inner_join(relative_prevalence, by=join_by(topic == V1))

write_xlsx(topic_word_prevalence, '../results/topic_words_prevalence.xlsx')

```

